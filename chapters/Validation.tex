\chapter{Validation}

\par The experiment was carried out over the first half of the Spring 2017 quarter at Cal Poly. 130 students signed up across 4 classes. By the end of the experiment, just over 3200 answers had been submitted to the application.

\section{Data Gathering}

\subsection{Midterm Scores}
\par In order to gauge the effectiveness of \textit{Polycommit}, we collected scores on quizzes and exams and aggregated the data to be analyzed. For \textbf{Introduction to Operating Systems}, data collection was extremely simple as all the quizzes and midterms were administered through an online exam. This allowed us to easily obtain data on how students performed on specific midterm questions that aligned with questions asked in \textit{Polycommit}.

\par However, the other classes were not as simple. The other classes administered paper midterms, and we did not have access to the papers as they were being graded. As such, we administered a secondary survey to all the students in the class asking them to input their midterm scores for various questions.

\subsection{Final Survey}
\par After the experiment was concluded, all participants were sent a link to a survey where they could give feedback about their overall experience using \textit{Polycommit}. As an incentive, any participant who filled out the survey received +3 Commitment and +50 points (equal to an additional entry in the gift card raffle). A total of 70 participants filled out the post-experiment survey.

\subsection{Limitations}
\par Certain tradeoffs were necessary in order to run this experiment.
\subsubsection{No Control Group} 
\par Most importantly, there is no true control group for this experiment. We decided that it would be more advantageous to offer the application to all students in each section of the classes. This simplified the onboarding process, as we could present the application to all members of the class at once, and allow anyone to sign up. It also prevented potential issues where students might perceive it as unfair that certain students were give access to a study resource and others were not. In addition, this allowed more students to participate in the experiment, giving us access to more data and more user feedback.

\par As such, it's important to note that the students who participated in the study \textbf{self-selected} to become part of the experiment. Students who \textbf{did not} use the app can't be considered a control group, because they are not a random selection from the whole population. Allowing students to self-select may have introduced confounding variables. For example, perhaps students who are confident in their study techniques would choose to not participate, making the average midterm scores higher for non-participants.

\subsubsection{Self-Reporting Midterm Scores}
\par As noted above, students were asked to input their midterm scores in an online form. We have no way of determining if this data was accurately entered; students may feel tempted to enter a higher score than they actually received. Note that we \textit{do} have access to the overall midterm scores, so the issue only arises when analyzing individual questions on the exams. Data that has been affected by this issue is clearly marked in the upcoming sections.

\section{Overall Results}
\subsection{Cramming vs. Commitment}
\par First, consider the relationship between a student's overall score on the midterm and their Commitment (\textbf{\hyperref[fig:comm_vs_score]{Figure \ref*{fig:comm_vs_score}}}). 
 
 \begin{figure}[ht]
 	\includegraphics[width=1.0\linewidth]{figures/commitment-data1}
 	\caption{Graph of Commitment versus the student's percentage score for all classes.}
 	\label{fig:comm_vs_score}
 \end{figure}

\par Note that we are showing the midterm percentage score, since the midterms in different classes have different total score values.
\par There is no clear correlation between a user's Commitment value and the score they got on the midterm. Thus, this data does not support the hypothesis that using spaced repetition to study for an exam improves the user's score.

\par  Next, we can compare the total number of questions answered by each user with their midterm score (\textbf{\hyperref[fig:cramming]{Figure \ref*{fig:cramming}}}). If students received a benefit from the Commitment system, we might expect to see \textit{less} of a correlation between total questions answered and midterm score. This is because if a user has a high number of questions answered but a low Commitment, this indicates they answered the questions all at once, in a "cram" session. According to our hypothesis, this would not help improve their long-term understanding of the course.

\begin{figure}[ht]
	\includegraphics[width=1.0\linewidth]{figures/cramming-data}
	\caption{Graph of Total Questions Answered vs. the student's score on the midterm.}
	\label{fig:cramming}
\end{figure}
 
 \par Next, consider the mean of all midterm scores between students that didn't answer \textit{any} questions vs. the ones that did. For all classes except CPE 471, students that didn't answer any questions in the app scored slightly higher on average than the students that didn't. The discrepancy in CPE 471 is most likely due to the smaller number of students that signed up for \textit{Polycommit}.
 
 \par Note that we define "Active Users" as any user with over 5 Commitment. 
  
  \begin{table}
 \begin{tabular}[h]{ r c c c}
  & \textbf{Non-users} & \textbf{All Users} & \textbf{Active Users} \\
  \hline
  \textbf{All Classes} & & & \\
  Midterm Average & 72.93 & 76.12 & 76.88 \\
  Population Size & 119 & 103 & 87 \\
  \hline
  \textbf{CPE 453} & & & \\
 Midterm Average & 74.38 & 77.56 & 77.40 \\
 Population Size & 14 & 46 & 42 \\
 \hline
 \textbf{CPE 464} & & & \\
 Midterm Average & 68.90 & 73.85 & 76.08 \\
 Population Size & 47 & 42 & 34 \\
 \hline
 \textbf{CPE 471} & & & \\
 Midterm Average & 87.38 & 82.39 & 83.83 \\
 Population Size & 34 & 7 & 4 \\
 \hline
 \textbf{MATH 244} & & & \\
 Midterm Average & 74.43 & 85.87 & 85.87 \\
 Population Size & 26 & 11 & 10 \\
 
\end{tabular}
\caption{Average midterm scores for different samples of the population. Note that an "Active User" is a user with more than 5 Commitment.}
\end{table}

\subsection{Individual Questions}
Certain questions on the midterm matched closely with the content of the questions that were repeated on \textit{Polycommit}. We can analyze midterm results to see if the students that drilled related problems in \textit{Polycommit} performed better on the related questions during the midterm.

\subsubsection{Introduction to Operating Systems}
One question that was shared between the midterm and \textit{Polycommit} was a question about the 4 conditions for deadlock. The question on the midterm asked students to choose the correct 4 conditions from a series of dropdowns, while the question on \textit{Polycommit} asked students to choose the \textit{incorrect} condition from the list. The 4 conditions for deadlock are:

\begin{enumerate}
	\item \textbf{Mutual exclusion}: At least one resource can only be
	held by one process at a time; this can result in other
	processes waiting for that resources
	\item \textbf{Hold and wait}: A process must be holding at least one
	resource while waiting for other resources (held by other
	processes)
	\item \textbf{No preemption}: Resources can only be released
	voluntarily by a process; Resources cannot be revoked
	\item \textbf{Circular wait}: A set of n waiting process $ {P_0, ..., P_n} $
	such that $ P_i $ is waiting for resources held by $ P_{(i+1)\%n} $
\end{enumerate}

\par The content of the question in \textit{Polycommit} and the question in the midterm are very similar. We can see if individuals who answered the Deadlock question correctly in \textit{Polycommit} had a higher score, on average, than the individuals who did not.

\begin{figure}[th!]
	\includegraphics[width=0.5\linewidth]{figures/deadlock-nonusers}
	\caption{Count of scores for students that did \textit{not} make an attempt on the Deadlock question in \textit{Polycommit}.}
	\label{fig:deadlock-no}
\end{figure}

\begin{figure}[h!b]
	\includegraphics[width=0.5\linewidth]{figures/deadlock-users}
	\caption{Count of scores for students that \textit{did} make an attempt on the Deadlock question in \textit{Polycommit}.}
	\label{fig:deadlock-yes}
\end{figure}

\par The mean score for \textbf{non-users} is 3.30, and the mean score for \textbf{users} is 3.31. Thus, using \textit{Polycommit} had a negligible effect on the average score of the participants. However, note that 23/26 \textbf{non-users} received a score greater than 3, while 34/37 \textbf{users} received a score greater than 3. This could reflect the fact that \textit{Polycommit} only referenced 3 out of the 4 conditions for deadlock, as the 4th condition in the problem was a false plant. Although all 4 conditions are listed in the "explanation" field for the problem, by default if a student answers the question correctly they are brought back to the course page. Thus, students who participated in \textit{Polycommit} were only drilled on 3 out of the 4 conditions for deadlock, which would explain the overall performance on the midterm.

\par Next, we will examine problems related to scheduling. Scheduling, in Operating Systems, is the process by which a multithreaded CPU decides which processes get to run at a certain time. Since scheduling problems are easy to generate and automatically grade, several such problems were included in \textit{Polycommit}. Below, we graph the amount of scheduling problems users answered in \textit{Polycommit} versus the score they got on the scheduling section of the midterm (See \textbf{\hyperref[fig:scheduling]{Figure \ref*{fig:scheduling}}}).

% TODO: also include graph where people got the polycommit question incorrect

\begin{figure}[h!b]
	\includegraphics[width=1.0\linewidth]{figures/scheduling}
	\caption{Scatter plot where the X axis is the amount of scheduling problems the user answered on \textit{Polycommit}, and the Y axis is the total score they received on the 4 scheduling questions of the midterm.}
	\label{fig:scheduling}
\end{figure}

\par As you can see, there is a clear correlation between consistently practicing scheduling questions and performing well on the exam. While many students did get full credit on the scheduling portion of the exam, the students who answered all 5 scheduling questions on Polycommit all scored perfectly on that section of the exam.


\subsubsection{Introduction to Computer Graphics}

\par Questions about \textbf{vector math}, \textbf{transformation matrices}, and \textbf{barycentric coordinates} were practiced in \textit{Polycommit}, and were subsequently tested on the midterm for Computer Graphics. Below is a table displaying the midterm performance of students who answered questions about each question on \textit{Polycommit}.

\begin{table}
	\begin{tabular}[h!]{ r c c }
		& \textbf{Did practice} & \textbf{Did not practice} \\
		\hline
		\textbf{Vector Math} & & \\
		Score & 98\% & 100\%  \\
		
		\hline
		\textbf{Transformation Matrices} & &  \\
		Score  & 92.1\% & 86.3\% \\
		
		\hline
		\textbf{Barycentric Coordinates} & &  \\
		Score  & 100\% & 97.7\% \\
		
	\end{tabular}
\caption{IMPORTANT: These midterm scores were self-reported, so their accuracy depends on the honesty of the reporting student.}
\end{table}

\subsubsection{Introduction to Computer Networks}

\par Questions about \textbf{IP Addressing}, \textbf{CRC Checksums} and \textbf{sliding window protocol} were practiced in \textit{Polycommit}, and were subsequently tested on the midterm for Computer Networks. Below is a table displaying the midterm performance of students who answered questions about each question on \textit{Polycommit}.

\begin{table}
	\begin{tabular}[h!]{ r c c }
		& \textbf{Did practice} & \textbf{Did not practice} \\
		\hline
		\textbf{IP Addressing} & & \\
		Score & 80\% & 77.4\%  \\
		
		\hline
		\textbf{CRC Checksums} & &  \\
		Score  & 100\% & 86.3\% \\
		
		\hline
		\textbf{Sliding Window Protcol} & &  \\
		Score  & 90\% & 76.9\% \\
		
	\end{tabular}
\caption{IMPORTANT: These midterm scores were self-reported, so their accuracy depends on the honesty of the reporting student.}
\end{table}

\section {User Feedback}

\par At the end of the experiment, we contacted all of the participants with a final survey that asked them questions about their usage of the app. It also included two free-response questions that asked users to offer suggestions about how the app could be improved.

\subsection{Rating Summary}

\par The full survey is viewable in \textbf{\hyperref[fig:survey1]{Figure \ref*{fig:survey1}}}. Users responded to several statements about the app using a standard Likert scale ranging from Strongly Disagree to Strongly Agree. A summary of the user responses is available at \textbf{\hyperref[fig:likert]{Figure \ref*{fig:likert}}}. Notably, a majority of users responded quite positively to the app.


\begin{figure}[h]
	\includegraphics[width=1.0\linewidth]{figures/likert}
	\caption{Survey responses from users of Polycommit.}
	\label{fig:likert}
\end{figure}

\subsection{User Self-Evaluation vs. Actual Scores}

\par One interesting result comes by grouping midterm scores by the user's response to "\textit{Polycommit} improved my scores on quizzes and exams" (See \textbf{\hyperref[fig:overconfidence]{Figure \ref*{fig:overconfidence}}}). Students who reported that \textit{Polycommit} did \textit{not} improve their scores scored substantially higher than students who did. One explanation for this is that students who were already confident about their studying habits or had prior knowledge of Operating Systems did not feel that \textit{Polycommit} had an impact on their grades, which are already high overall.

\begin{figure}[h]
	\includegraphics[width=1.0\linewidth]{figures/improved-vs-score}
	\caption{Mean midterm score grouped by response to final survey question "\textit{Polycommit} improved my scores on quizzes and exams."}
	\label{fig:overconfidence}
\end{figure}


\subsection{Free Response Summary}
\par Before viewing the results, we predicted some categories that user responses might fall into. These categories are:

\begin{enumerate}
	\item Problems or confusions with the user interface
	\item Questions were not correct or were confusing
	\item Questions did not represent test material
	\item App needed more gamification, excitement, or incentive to use
	\item Other
\end{enumerate}

\par The total number of responses that fell under each category are:

\vspace{1.0cm}

\begin{tabular}{ r c }
	\textbf{Category} & \textbf{\# Responses} \\
	Problems or confusions with the user interface & 26 \\
	Questions were not correct or were confusing  & 9 \\
	Questions did not represent test material  & 5 \\
	App needed more gamification, excitement, or incentive to use  & 1 \\
	Other  & 29 \\
\end{tabular}

\vspace{2.0cm}

\par The full list of responses is available below. Note that answers \textit{are} shown when a question is closed, so feedback asking for answers to be displayed is filed under "UI Issues" since the problem is essentially that the app failed to present the information in a way that users could find it.

\input{chapters/UserFeedbackList}



 